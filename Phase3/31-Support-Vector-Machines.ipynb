{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "_Adapted from Yish's interpretation of Chapter 9 of Introduction to Statistical Learning in R_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier\n",
    "\n",
    "SVMs approach the classification problem in a direct way - __we try and find a plane that separates the classes in the feature space__\n",
    "\n",
    "If we cannot, we do one of the two things:\n",
    "\n",
    "- We __soften__ what we mean by \"separates\" \n",
    "- We __enlarge__ the feature space so that the separation is possible\n",
    "\n",
    "__Notes on Terminology__:\n",
    "\n",
    "- Support vector machines are sometimes used as a general method that incorporate maximal margin classifier, support vector classifiers etc. However, strictly by definition, support vector machine is a support vector classifier utilized with non-linear kernel. \n",
    "\n",
    "> \"When the support vector classifier is combined with a non-linear kernel such as (9.22), the resulting classifier is known as a support vector machine.\" -- P366, ISLR\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a boundary hyperplane in a two dimensional space:\n",
    "\n",
    "<img src=\"images/exampleboundary.png\" width=500>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximal Margin Classifier\n",
    "\n",
    "SVM tackles the problem of classification directly, in the sense that it does not compute a probabilistic model. Instead, it constructs a hyperplane to directly separate the classes. \n",
    "\n",
    "For example:\n",
    "\n",
    "![](images/manyboundaries.png)\n",
    "\n",
    "However, the problem with this approach is that we can come up with infinite number of such hyperplanes as we can tilt the line back and forth and it would still serve the same purpose.\n",
    "\n",
    "__Therefore, we are using the hyperplane such that the it would be the farthest from training observations from either side__. The intuition behind it is that if we have chosen a hyperplane that is far from the training observations, it would be far for the testing observations as well. <br>\n",
    "\n",
    "The distance between the training observations and the hyperplane is called the _margin_, and the classifier aims to find the maximal margin from the hyperplane that separates the training examples:\n",
    "\n",
    "<img src=\"images/bestboundary.png\" width=500>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Margin Classifier\n",
    "\n",
    "Even though the maximal margin classifier sounds like an intuitive idea and not too difficult to optimize for, it might not be desirable under two circumstances:\n",
    "\n",
    "1. It will be sensitive to individual training observations\n",
    "2. The algorithm will not converge if the training observations cannot be linearly separated.\n",
    "\n",
    "![](images/softboundary.png)\n",
    "\n",
    "What happens if we cannot come up with a hyperplane that perfectly separates the training observations like the ones above? The first solution is the soft margin classifier, where we can loosen up our definition of the margin. \n",
    "\n",
    "<img src=\"images/withoutboundary.png\" width=500>\n",
    "\n",
    "__Rather than seeking the largest possible margin so that every observation is not only on the correct side of the hyperplane but also on the correct side of the margin, we instead allow some observations to be on the incorrect side of the margin, or even on the incorrect side of the hyperplane.__\n",
    "\n",
    "![](images/errortolerance.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the hyperparameter ε is known as the slack variables, which dictate how many training observations we allow to violate the rule of margins or even the hyperplane. The amount of slack is bounded by C accordingly.\n",
    "\n",
    "The parameter εi tells us where the ith training observation is located. \n",
    "- If εi = 0, then we say the ith training observation is on the correct side of the margin;\n",
    "- If εi > 0, then we say it has violated the margin\n",
    "- If εi > 1, then it is on the wrong side of the hyperplane\n",
    "\n",
    "The value C is the sum of ε across all i training observations. The parameter C controls the bias-variance tradeoff of the statistical technique. A high value of C meaning we are more tolerant of the violation, which in turn might give us a model that has high bias but low variance; however, a low value of C indicates low tolerance of the violation, potentially giving us more variance but less bias. \n",
    "\n",
    "__How do we determine the ideal value of C?__\n",
    "\n",
    "![](images/impactofchyperparameter.png)\n",
    "\n",
    "__Note!__\n",
    "\n",
    "In scikit-learn implementation, `c` is defined as the inverse. A higher value of `c` is a smaller regularization or smaller penalty, whereas a lower value of `c` is a higher penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note!__\n",
    "It is important to point out that in the support vector classfier (or SVM) in general, only the vectors on the margins are used for classification. They are called the __\"Support Vectors\"__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"The Kernel Trick\"\n",
    "\n",
    "Sometimes we have training data that are not able to be separated even with softened margin:\n",
    "\n",
    "![](images/needforkerneltrick.png)\n",
    "\n",
    "The intuition to find the optimal fit is called feature space expansion:\n",
    "\n",
    "- First, we __enlarge__ the feature space through the use of kernel\n",
    "- Fit a support vector classifier in the enlarged space \n",
    "- This results in nonlinear decision boundaries in the original space \n",
    "\n",
    "\n",
    "<img src=\"images/nonlinearring.png\" width=500>\n",
    "\n",
    "<img src=\"images/nonlinearringin3d.png\" width=500>\n",
    "\n",
    "Why do we know that enlarging the feature space makes the data more linearly separable? [Cover's Theorem](https://en.wikipedia.org/wiki/Cover%27s_theorem).\n",
    "\n",
    "Another view:\n",
    "\n",
    "![](images/kernel_trick_hyperdimensional.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation & Performance Comparison\n",
    "\n",
    "Docs: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data_banknote_authentication.csv', header = None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our data needs column names\n",
    "headers = [\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\", \"Class\"]\n",
    "df.columns = headers\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y, then train test split\n",
    "X = df.drop('Class', axis=1)  \n",
    "y = df['Class'] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First: Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "tic = time() #timing!\n",
    "\n",
    "svc_linear = SVC(kernel='linear', C=1)\n",
    "svc_linear.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = svc_linear.predict(X_train)\n",
    "y_pred_test = svc_linear.predict(X_test)\n",
    "\n",
    "toc = time()\n",
    "print(f\"Run time is {toc-tic} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# how'd we do?\n",
    "from sklearn.metrics import classification_report, plot_confusion_matrix , accuracy_score\n",
    "\n",
    "print(classification_report(y_test, y_pred_test)) \n",
    "print(f\"Train accuracy: {accuracy_score(y_train, y_pred_train):.4f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_pred_test):.4f}\")\n",
    "\n",
    "plot_confusion_matrix(svc_linear, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now: RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time() #timing!\n",
    "\n",
    "svc_rbf = SVC(kernel='rbf', C=1, gamma='scale') # using all default values here\n",
    "svc_rbf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = svc_rbf.predict(X_train)\n",
    "y_pred_test = svc_rbf.predict(X_test)\n",
    "\n",
    "toc = time()\n",
    "print(f\"Run time is {toc-tic} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# how'd we do?\n",
    "print(classification_report(y_test, y_pred_test)) \n",
    "print(f\"Train accuracy: {accuracy_score(y_train, y_pred_train):.4f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_pred_test):.4f}\")\n",
    "\n",
    "plot_confusion_matrix(svc_rbf, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And a Polynomial Kernel for good measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time() #timing!\n",
    "\n",
    "svc_poly = SVC(kernel='poly', C=1, gamma='scale', degree=3) # using mostly default values here\n",
    "svc_poly.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = svc_poly.predict(X_train)\n",
    "y_pred_test = svc_poly.predict(X_test)\n",
    "\n",
    "toc = time()\n",
    "print(f\"Run time is {toc-tic} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how'd we do?\n",
    "print(classification_report(y_test, y_pred_test)) \n",
    "print(f\"Train accuracy: {accuracy_score(y_train, y_pred_train):.4f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_pred_test):.4f}\")\n",
    "\n",
    "plot_confusion_matrix(svc_poly, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting C?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [.01, 1, 100]: \n",
    "    svc_c = SVC(kernel='linear', C=c, gamma='scale') # going linear again\n",
    "    svc_c.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_train = svc_c.predict(X_train)\n",
    "    y_pred_test = svc_c.predict(X_test)\n",
    "\n",
    "    # how'd we do?\n",
    "    print(\"-----\")\n",
    "    print(f'Results at C = {c}')\n",
    "    print(classification_report(y_test, y_pred_test)) \n",
    "    print(f\"Train accuracy: {accuracy_score(y_train, y_pred_train):.4f}\")\n",
    "    print(f\"Test accuracy: {accuracy_score(y_test, y_pred_test):.4f}\")\n",
    "\n",
    "    plot_confusion_matrix(svc_c, X_test, y_test)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros \n",
    "\n",
    "- Good for datasets with more variables than observations\n",
    "- Robust against outliers\n",
    "- Good performance\n",
    "- Good off-the-shelf model in general for several scenarios\n",
    "- Can approximate complex non-linear functions\n",
    "\n",
    "## Cons \n",
    "\n",
    "- Long training time required\n",
    "- Tuning required to determine optimal kernel for non-linear SVMs\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Scaled features\n",
    "- Null values filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [An Idiot's Guide to Support Vector Machines (SVMs) from MIT](https://web.mit.edu/6.034/wwwbob/svm.pdf)\n",
    "- [Machine Learning Mastery's Post on Support Vector Machines for Machine Learning](https://machinelearningmastery.com/support-vector-machines-for-machine-learning/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "193.182px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
